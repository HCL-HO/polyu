# COMP5541 Exam
## Section A: Multiple Choice Questions (30 marks)
(Only one option is correct in each question.)

### 1. (3 marks) Which method can mitigate gradient diminishing?
(a) using ReLU activation instead of Sigmoid.

(b) applying gradient clipping.

(c) adding more neural layers.

(d) removing more bias neurons.

### 2. (3 marks) Which of the following is true about the stochastic gradient descent with momentum?

(a) it has no hyperparameters.

(b) it can be regarded as regularization.

(c) it leads to overfitting.

(d) none of the above

### 3. (3 marks) Which of the following is true about backpropagation?

(a) if the  neural  network  has $N$ trainable  parameters,  backpropagation  is  an $O(N^2)$ operation.

(b) the learning rate will be updated using backpropagation.

(c)  if a specific learnable parameter always has a derivative of zero, it will not beupdated in backpropagation.

(d) none of the above.

### 4. (3 marks) Your network for object classification gets a high training error.  Which of the following is promising to improve your model?

(a) adding more neural layers.

(b) collecting more training data.

(c) applying L1 or L2 regularization.

(d) raining fewer epochs.

### 5. (3 marks) Your  network  for  object  classification  gets  a  training  accuracy  of  95% while the test accuracy is only 25%.  Which of the following is promising to improveyour model?

(a) applying L2 regularization.

(b) reducing the number of training samples.

(c) replacing ReLU by Leaky ReLU.

(d) none of the above.

### 6. (3 marks) Which of the following is true about CNNs for image analysis?

(a) pooling layers can reduce the spatial resolution of feature maps generated by CNNs.

(b) CNNs can only process square images.

(c) CNNs can be used for contrastive learning, but fully-connected layers cannot.

(d) filters in earlier CNN layers tend to learn high-level semantic features.

### 7. (3 marks) Which of the following data set should not be used to update the learning rate of a neural network?

(a) test set

(b) validation set

(c) training set

(d) all above

### 8. (3 marks) Which of the following statements is true?

(a) variational autoencoder requires the reparameterization trick to minimize the KL loss.

(b) ransformers are recurrent neural networks.

(c) graph convolutional neural layer cannot extract pixel local patterns from images.

(d) stacking fully connected layers and ReLU functions as many as possible can approximate universal functions.

### 9. (3 marks) Which of the following statements about reinforcement learning is false?

(a) different training episodes can have different number of observations.

(b) same as unsupervised learning methods, reinforcement learning needs super-vision signals.

(c) policy gradient method learns the gradients of network parameters directly.

(d) the collected raw rewards should be discounted for better optimization.

### 10. (3 marks) Which of the following can be a valid activation function (elementwise non-linearities) to train a neural network?

(a) $f(x)=0.8x+0.2$

(b) $f(x)=-\min(0.8, x)$

(c) $f(x)= \left\{\begin{matrix}
 \max(x,0.2x)&,x>=0\\ 
 \min(x,0.2x)&,x<0
\end{matrix}\right.$

(d) none of the above.

## Section B: Backpropagation (36 marks)


As shown in the figure below, a siamese neural network consists of twin networks whichaccept distinct inputs but share the same weights.  The outputs of the twin networks areusually joined later on by more layers.  Let’s assume we have a siamese neural network,as defined below:

$z_1=W_1x, x\in R_{D\times 1}, z_1\in R_{K\times 1}$

$a_1 = ReLU(z_1)$

$z_2=W_1x'$

$a_2=ReLU(z_2)$

$a=a_1-a_2$

$z_3=W_2a$

$\hat{y}=\sigma(z_3)$

$L = -\bar{y}\log \hat{y}-(1-\bar{y})\log (1-\hat{y})$

### 11. (6 marks) What are the shapes of $W_1$ and $W_2$?

### 12. (6 marks) What is the expression for $\frac{\partial L}{\partial z_3}$?
### 13. (6 marks) What is the expression for $\frac{\partial z_3}{\partial a}$?
### 14. (6 marks) What is the expression for $\frac{\partial a}{\partial z_1}$?
### 15. (6 marks) What is the expression for $\frac{\partial a}{\partial z_2}$?
### 16. (6 marks) What is the expression for $\frac{\partial z_1}{\partial W_1}$?

## Section C: Convolutional Neural Networks (34 marks)

As shown in the table below, a convolutional neural network is defined by the layers inthe left column.  Fill in the shape of output features and the number of parameters ineach layer.**For simplicity, there is no bias term in the whole network.**  Writethe shape in the format $(H, W, C)$, where $H, W, C$ are the height, width and channeldimensions, respectively.

- Conv $(K\times K \times M)$ denotes a convolutional layer with $M$ filters of height and widthequal to $K$, and with stride of $1$ and no padding.
- Pool $(S\times S)$ denotes an $S\times S$ max-pooling layer with stride of $S$ and no padding.
- Flatten denotes reshaping the tensor into a vector.
- Dense$(T)$ denotes a fully-connected layer with $T$ output neurons.

<table>
    <tr>
        <td>layer</td>
        <td>shape of features</td>
        <td>number of parameters</td>
    </tr>
    <tr>
        <td>Input</td>
        <td>32×32×3</td>
        <td>0</td>
    </tr>
    <tr>
        <td>Conv(5×5×8)</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Leaky ReLU</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Pool(2 × 2)</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Conv(3×3×16)</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Leaky ReLU</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Pool(2×2)</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Flatten</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Dense(10)</td>
        <td></td>
        <td></td>
    </tr>
</table>

### 17. (16 marks) Fill in the table above.
### 18. (4 marks) The network is designed as an image classification system.  What is thenon-linear function of the last layer?

### 19. (4 marks) State the cross entropy loss function in terms of $y_i$ and $\bar{y_i}$, where $y_i$ is the $i^{th}$ dimension of the network prediction an $\bar{y_i}$ is the corresponding ground truth label.

### 20. (5 marks) If we change the training data by flipping all images upside down whilekeeping test data untouched, and then train on the training split.  Would the performanceof the network be similar to the network trained with original data and why?

### 21. (5 marks) If we change all Conv layers to fully-connected layers with a large numberof hidden neurons, will the new network require more images to train?  and why?