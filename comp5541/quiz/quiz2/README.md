# COMP5541 Quiz 2

### 1. The XOR problem can be solved by logistic regression model because it has a non-linear function Sigmoid. (5 points)

A. Yes

B. No

### 2. If the output layer of a neural network does not have a non-linear function, it cannot learn complicated non-linear relationships between X and y. (5 points)

A. Yes

B. No

### 3. The XOR problem cannot be solved by a neural network which has only one hidden layer. (5 points)

A. Yes

B. No

### 4. Because the bias term of the last layer (close to output y) of a neural network is not directly connected with any neurons in early layers, that bias term will not be updated during training. (5 points)

A. Yes

B. No

### 5. In back-propagation, the values of hidden neurons are not useful to compute the gradients of parameters. (5 points)

A. Yes

B. No

### 6. It is better to use Sigmoid function than ReLU in each hidden layer, because Sigmoid can model much more complex relationships, while ReLU is a simple piece-wise linear function. (5 points)

A. Yes

B. No

### 7. Given minibatch data points for training, the estimated gradient in each iteration is likely to have a high variance because the minibatch data may be more likely to be affected by noisy points. (5 points)

A. Yes

B. No

### 8. If we use gradient descent with momentum to compute the gradients, it cannot consider the gradients of multiple consecutive minibatch data points. (5 points)

A. Yes

B. No


### 9. If we use L2 loss function in the neural network, we have to apply L2 regularization for parameters W (5 points)

A. Yes

B. No

### 10. Compared with L1 regularization, L2 regularization is more likely to obtain sparse parameters W (5 points)

A. Yes

B. No

### 11. If we use ReLU or LeakyReLU for all hidden neurons and the simple Sigmoid function before the output neuron, then the output neuron is a complex piece-wise linear function of the input X. (5 points)

A. Yes

B. No

### 12. Because the kernels of a convolutional neural layer are much smaller than the dimensions of input feature maps, the neuron obtained from the dot-product is not a linear combination of the entire input feature maps. (5 points)

A. Yes

B. No

### 13. The convolutional neural layer can detect repeatable visual patterns from the entire image. (5 points)

A. Yes

B. No

### 14. Given an image patch and a kernel, if the obtained neuron after dot-product has a small value, it means that image patch is very likely to have a similar pattern with the kernel. (5 points)

A. Yes

B. No

### 15. Max-pooling layer is able to reduce network computation and overfitting. (5 points)

A. Yes

B. No

### 16. Dense neural networks are different from convolutional neural networks (CNNs) because CNNs cannot be a piece-wise linear function. (5 points)

A. Yes

B. No

### 17. Because the hidden states of a vanilla RNN unit can be a function of all previous time steps, it can always learn all patterns over long sequences. (5 points)

A. Yes

B. No

### 18. Because STM unit uses both hidden states and cell states to capture the sequential patterns, therefore it can always learn all features from unlimited long sequences. (5 points)

A. Yes

B. No

### 19. The Transformer network is invented by the paper "Attention is all you need", therefore the dense layers and convolutional neural layers are no longer needed to solve any problems. (5 points)

A. Yes

B. No

### 20. Because the stock prices can be formulated as a sequential data flow, the LSTM unit must be able to accurately predict the stock prices for the future only from the historical stock prices. (5 points)

A. Yes

B. No

## Solution
1. No
2. No
3. No
4. No
5. No
6. No
7. Yes
8. No
9. No
10. No
11. No
12. No
13. Yes
14. No
15. Yes
16. No
17. No
18. No
19. No
20. No
